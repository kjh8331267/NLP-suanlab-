{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"15 GPT2.ipynb","provenance":[{"file_id":"1cyRyTv3BZHahrjmbvnNW0tZkFDT0lM-G","timestamp":1658468707179}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zBjBNQX8kQfh"},"source":["# GPT(Generative Pre-trained Transformer) 2\n","\n","* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"]},{"cell_type":"markdown","metadata":{"id":"gKeqNH_dkTmT"},"source":["* OpenAI에서 GPT 모델 제안\n","* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n","* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n","\n","* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n","* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHkbiAq4cG0H","executionInfo":{"status":"ok","timestamp":1658468745510,"user_tz":-540,"elapsed":17531,"user":{"displayName":"김지현","userId":"05840969086042947577"}},"outputId":"93900c9d-c504-4dc1-e457-e3208c2f09a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"sDCr0YqjbfLJ"},"source":["## 라이브러리"]},{"cell_type":"code","metadata":{"id":"_ixYBCR8bguE"},"source":["!pip install transformers==2.11.0\n","!pip install tensorflow==2.2.0\n","!pip install sentencepiece==0.1.85\n","!pip install gluonnlp==0.9.1\n","!pip install mxnet==1.6.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPhczTnFjsG4"},"source":["## 데이터 다운로드\n","\n","* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"]},{"cell_type":"code","metadata":{"id":"hyCKy2LtjsG7"},"source":["!mkdir -p gpt2\n","!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pGgee8pjsHB"},"source":["import os\n","import numpy as np\n","import gluonnlp as nlp\n","from gluonnlp.data import SentencepieceTokenizer\n","from nltk.tokenize import sent_tokenize\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from transformers import TFGPT2LMHeadModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROOajn6VIzgy"},"source":["## 사전 학습 모델\n","\n","* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"]},{"cell_type":"code","metadata":{"id":"yoGiYGG1jsHJ"},"source":["!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -0 gpt_ckpt.zip\n","!upzip -o gpt_ckpt.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fjeaDUNjsHP"},"source":["class GPT2Model(tf.keras.Model):\n","    def __init__(self, dir_path):\n","        super(GPT2Model, self).__init__()\n","        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n","\n","    def call(self, inputs):\n","        return self.gpt2(inputs)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qlmm2I0jsHV"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","gpt_model = GPT2Model(BASE_MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5ilayG3jsHc"},"source":["BATCH_SIZE = 16\n","NUM_EPOCHS = 10\n","MAX_LEN = 30\n","TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","\n","tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n","nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                       mask_token = None,\n","                                       sep_token = None,\n","                                       cls_toeken = None,\n","                                       unknown_token = '<unk>',\n","                                       padding_token = '<pad>',\n","                                       bos_token = '<s>',\n","                                       eos_token = '<\\s>')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaFAxan-jsHg"},"source":["def tf_top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_value = 99999):\n","    _logits = logits.numpy()\n","    top_k = min(top_k, logits.shape[-1])\n","    if top_k > 0:\n","        indices_to_remove = logit < tf.math.top_k(logits, top_k)[0][..., -1, None]\n","        _logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits = tf.sort(logits, direction = 'DESCENDING')\n","        sorted_indices = tf.argsort(logits, direction = 'DESCENDING')\n","        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis = -1), axis = -1)\n","\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\n","        indeices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n","\n","        _logits[indices_to_remove] = filter_value\n","\n","    return tf.constant([_logits])\n","\n","def generate_sentence(seed_word, model, max_step = 100, greedy = False, top_k = 0, top_p = 0.):\n","    sentence = seed_word\n","    toked = tokenizer(sentence)\n","\n","    for _ in range(max_step):\n","        input_ids = tf.constant([vocab[vocab.bos_token], ]+ vocab[toked])[None, :]\n","        outputs = model(input_ids)[:, -1, :]\n","        if greedy:\n","            gen = vocab.to_tokens(tf.argmax(outputs, axis = -1).numpy().tolist()[0])\n","        else:\n","            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k = top_k, top_p = top_p)\n","            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n","        if gen == '<\\s>':\n","            break\n","        sentence += gen.replace('-', ' ')\n","        toked = tokenizer(sentence)\n","\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6bhahzWjsHl"},"source":["generate_sentence('일부', gpt_model, greedy = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW5jmfiejsHr"},"source":["generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M5yWJea3I7-n"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"CVWJaywYjsHw"},"source":["DATA_IN_PATH = './gpt2/'\n","TRAIN_DATA_FILE = 'finetune_data.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVXUVGH5jsH0"},"source":["sentences = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE).readlines()]\n","\n","input_data = []\n","output_data = []\n","\n","for sentence in sentences:\n","    tokens = [vocab[vocab.bos_token], ] + vocab[tokenizer(sentence)] + [vocab[vocab.eos_token], ]\n","    input_data.append(tokens[:-1])\n","    output_data.append(tokens[1:])\n","\n","input_data = pad_sequences(input_data, MAX_LEN, value = vocab[vocab.padding_token])\n","output_data = pad_sequences(output_data, MAX_LEN, value = vocab[vocab.padding_token])\n","\n","input_data = np.array(input_data, dtype = np.int64)\n","output_data = np.array(output_data, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4fmyXIZJMxm"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"NDIvpflCjsH5"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n","                                                            reduction = 'none')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","def accuracy_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n","    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n","    pred *= mask\n","    acc = train_accuracy(real, pred)\n","\n","    return tf.reduce_mean(acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxW9BUs-jsH9"},"source":["gpt_model.compile(loss = loss_function,\n","                  optimizer = tf.keras.optimizers.Adam(1e-4),\n","                  metrics = [accuracy_function])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McrNa1eEjsIC"},"source":["history = gpt_model.fit(input_data, output_data,\n","                        batch_size = BATCH_SIZE, epochs = NUM_EPOCHS,\n","                        validation_split = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFJHOJDqjsIG"},"source":["DATA_OUT_PATH = './data_out'\n","model_name = 'tf2_gpt2_finetuned_model'\n","\n","save_path = os.path.join(DATA_OUT_PATH, model_name)\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","\n","gpt_model.gpt2.save_pretrained(save_path)\n","\n","loadded_gpt_model = GPT2Model(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBYN8CuxjsIK"},"source":["generate_sentence('일부', gpt_model, greedy = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeP5zGxHjsIN"},"source":["generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BZEEq4mIMhr"},"source":["# GPT2 네이버 영화 리뷰 분류"]},{"cell_type":"markdown","metadata":{"id":"ZTXFkRQYxGa0"},"source":["## 데이터 다운로드"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ijkw_0U2xGa-"},"source":["import re\n","import urllib.request\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-white')\n","\n","from transformers import TFGPT2Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNs8XHaUxGbQ"},"source":["tf.random.set_seed(111)\n","np.random.seed(111)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcbcRQKwxGbW"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"wpABh-81xGbW"},"source":["BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","VALID_SPLIT = 0.1\n","SENT_MAX_LEN = 39"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqPVUEwjxGbb"},"source":["TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","\n","tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                               mask_token = None,\n","                                               sep_token = '<unused0>',\n","                                               cls_toeken = None,\n","                                               unknown_token = '<unk>',\n","                                               padding_token = '<pad>',\n","                                               bos_token = '<s>',\n","                                               eos_token = '<\\s>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0I6dM15ym7uK"},"source":["* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"]},{"cell_type":"code","metadata":{"id":"IetCxzkbxGbf"},"source":["train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n","test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n","\n","train_data = pd.read_table(train_file)\n","test_data = pd.read_table(test_file)\n","\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_ZCDWgskiRp"},"source":["train_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVnAFFU-kiny"},"source":["test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lF8f3VcJxGbj"},"source":["def clean_text(text):\n","    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", text)\n","\n","    return text_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuAoVmTGxGbo"},"source":["train_data_sents = []\n","train_data_labels = []\n","\n","for train_sent, train_label in train_data[['document', 'label']].values:\n","    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([train_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    train_data_sents.append(tokens)\n","    train_data_labels.append(train_label)\n","\n","train_data_sents = np.array(train_data_sents, dtype = np.int64)\n","train_data_labels = np.array(train_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4w_U2EMQxGbs"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"5JYb6XjgxGbu"},"source":["class TFGPT2Classifier(tf.keras.Model):\n","    def __init__(self, dir_path, num_class):\n","        super(TFGPT2Classifier, self).__init__()\n","\n","        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n","        self.num_class = num_class\n","\n","        self.dropout = tf.keras.layers.Dropout(self.gpt2. config.summary_first_dropout)\n","        self.classifier = tf.keras.layers.Dense(self.num_class,\n","                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = self.gpt2.config.initializer_range),\n","                                                name = 'classifier')\n","        \n","    def call(self, inputs):\n","        outputs = self.gpt2(inputs)\n","        pooled_output = outputs[0][:, -1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self. classifier(pooled_output)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oUfrW5TxGby"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","cls_model = TFGPT2Classifier(dir_path = BASE_MODEL_PATH, num_class = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OsxKKImxGb1"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate = 6.25e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRF8D388xGb5"},"source":["model_name = 'tf2_gpt2_naver_movie'\n","\n","es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2)\n","\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} directory already exists\\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok = True)\n","    print(\"{} directory already complete\\n\".format(checkpoint_dir))\n","\n","cp_callback = ModelCheckpoint(checkpoint_path,\n","                              monitor = 'val_accuracy',\n","                              verbose = 1,\n","                              save_best_only = True,\n","                              save_weights_only = True)\n","\n","history = cls_model.fit(train_data_sents, train_data_labels,\n","                        epochs = NUM_EPOCHS,\n","                        batch_size = BATCH_SIZE,\n","                        validation_split = VALID_SPLIT,\n","                        callbacks = [es_callback, cp_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGj4h0l3xGb9"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Loss', 'Validation Loss'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7x-FC6BDxGcB"},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Accuracy', 'Validation Accuracy'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKJx63kSxGcF"},"source":["## 모델 평가"]},{"cell_type":"code","metadata":{"id":"VywcseLrxGcH"},"source":["test_data_sents = []\n","test_data_labels = []\n","\n","for test_sent, test_label in test_data[['document', 'label']].values:\n","    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([test_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    test_data_sents.append(tokens)\n","    test_data_labels.append(test_label)\n","\n","test_data_sents = np.array(test_data_sents, dtype = np.int64)\n","test_data_labels = np.array(test_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wj3dRljzxGcP"},"source":["cls_model.load_weights(checkpoint_path)\n","cls_model.evaluate(test_data_sents, test_data_labels, batch_size = 1024)"],"execution_count":null,"outputs":[]}]}