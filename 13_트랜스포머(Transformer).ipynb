{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PQSnsGeA2OH"
   },
   "source": [
    "# 트랜스포머 (Transformer)\n",
    "\n",
    "* 참고: https://wikidocs.net/31379"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbQ-h_XxBAiq"
   },
   "source": [
    "* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n",
    "* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n",
    "* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDiFPIdUBBS2"
   },
   "source": [
    "## 포지셔널 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLqHf_4SEWoa"
   },
   "source": [
    "* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n",
    "* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n",
    "* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n",
    "* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1659403330969,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "SiO5c_HIFBAk"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(dim, sentence_length):\n",
    "    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim) for pos in range(sentence_length) for i in range(dim)])\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2]) # 짝수 -> sin\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2]) # 홀수 -> cos\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "099gUUxhAgy3"
   },
   "source": [
    "## 레이어 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCdips98yPuH"
   },
   "source": [
    "*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n",
    "*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1659403507860,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "TSJjxF86Aeg3"
   },
   "outputs": [],
   "source": [
    "def layer_norm(inputs, eps = 1e-6):\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    mean = tf.keras.backed.mean(inputs, [-1], keepdims = True)\n",
    "    std = tf.keras.backed.std(inputs, [-1], keepdims = True)\n",
    "    beta = tf.Variable(tf.zeros(feature_shape), trainable = False)\n",
    "    gamma = tf.Variable(tf.ones(feature_shape), trainable = False)\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1659403508877,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "km9ORxIun-MU"
   },
   "outputs": [],
   "source": [
    "def sublayer_connection(inputs, sublayer, dropout = 0.2):\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Droupout(dropout)(sublayer))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppb7IxJ3diMC"
   },
   "source": [
    "## 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JaU6MHgy9V2"
   },
   "source": [
    "\n",
    "\n",
    "*   트랜스포머 모델의 핵심이 되는 부분\n",
    "*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n",
    "  1.   multi-head attention\n",
    "      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n",
    "      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n",
    "      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n",
    "  2.   self attention\n",
    "      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n",
    "      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n",
    "      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n",
    "      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRyL0KDXi6ej"
   },
   "source": [
    "### scaled-dot product attention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HtmcgRR3Cr-"
   },
   "source": [
    "* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n",
    "* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n",
    "* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1659404117589,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "ALEMzi4fdiSQ"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, masked = False):\n",
    "    key_dim_size = float(key.get_shape().as_list()[-1])\n",
    "    key = tf.transpose(key, perm = [0, 2, 1])\n",
    "\n",
    "    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n",
    "\n",
    "    if masked:\n",
    "        diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # 아래 삼각행렬 남기고 위 삼각행렬은 padding 처리\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "        paddings = tf.ones_like(masks)*(-2**30) # 0이 아닌 아주 작은 값으로\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "\n",
    "    attention_map = tf.nn.softmax(outputs)\n",
    "    return tf.matmul(attention_map, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr20BxvVi-8b"
   },
   "source": [
    "### multi-head attention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb5qflUH14-H"
   },
   "source": [
    "* multi-head attention의 구현 과정\n",
    "  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n",
    "  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n",
    "  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n",
    "  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1659404314272,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "ooc3FAdQi_Gz"
   },
   "outputs": [],
   "source": [
    "def multi_head_attention(query, key, value, num_units, heads, masked = False):\n",
    "    query = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(query)\n",
    "    key = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(key)\n",
    "    value = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(value)\n",
    "\n",
    "    query = tf.concat(tf.split(query, heads, axis = -1), axis = 0)\n",
    "    key = tf.concat(tf.split(key, heads, axis = -1), axis = 0)\n",
    "    value = tf.concat(tf.split(value, heads, axis = -1), axis = 0)\n",
    "\n",
    "    attention_map = scaled_dot_product_attention(query, key, value, masked)\n",
    "    attn_outputs = tf.concat(tf.split(attention_map, heads, axis = 0), axis = -1)\n",
    "    attn_outputs = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(attn_outputs)\n",
    "\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78Zn5-fYITD4"
   },
   "source": [
    "## 포지션-와이즈 피드 포워드 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xxeG2xvo3ZN"
   },
   "source": [
    "\n",
    "\n",
    "*   multi-head attention의 결과인 행렬을 입력받아 연산\n",
    "*   일반적인 완전 연결 신경망(Dense layer)를 사용\n",
    "*   position-wise FFNN은 인코더와 디코더에 모두 존재\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1659404427124,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "0tSFd5OaITJ0"
   },
   "outputs": [],
   "source": [
    "def feed_forward(inputs, num_units):\n",
    "    feature_shape = inputs.get_shape()[-1]\n",
    "    inner_layer = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuccViYgBK6v"
   },
   "source": [
    "## 인코더\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG3MH0n1JVLz"
   },
   "source": [
    "* 인코더는 하나의 어텐션을 사용\n",
    "  + encoder self-attention (multi-head self-attention과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1659404689931,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "m5T0pzBoAnn3"
   },
   "outputs": [],
   "source": [
    "def encoder_module(inputs, model_dim, ffn_dim, heads):\n",
    "    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "    return outputs\n",
    "\n",
    "def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcgHRcTEBQqg"
   },
   "source": [
    "## 디코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNj-6FLQwT4-"
   },
   "source": [
    "* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n",
    "  1. masked decoder self-attention\n",
    "  2. encoder-decoder attention\n",
    "  3. position-wise FFNN\n",
    "\n",
    "* 디코더에서는 2종류의 어텐션을 사용\n",
    "  1.   masked decoder self-attention\n",
    "    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n",
    "    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n",
    "    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n",
    "  2.   encoder-decoder attention\n",
    "    *   앞서 설명한 multi-head attention과 동일\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1659404950319,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "2B05wr7aARcT"
   },
   "outputs": [],
   "source": [
    "def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n",
    "    masked_self_attn = sublayer_connection(inputs,\n",
    "                                           multi_head_attention(inputs, inputs, inputs,\n",
    "                                                                model_dim, heads, masked = True))\n",
    "    \n",
    "    self_attn = sublayer_connection(masked_self_attn,\n",
    "                                    multi_head_attention(masked_self_attn,\n",
    "                                                         encoder_outputs,\n",
    "                                                         encoder_outputs,\n",
    "                                                         model_dim, heads))\n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "    return outputs\n",
    "\n",
    "def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtztlyUB1ERS"
   },
   "source": [
    "## 트랜스포머를 활용한 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CGUIAzv6eWs"
   },
   "source": [
    "### konlpy 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae0mHT49v5gy"
   },
   "source": [
    "*    한글을 처리하기 위해 konlpy 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6185,
     "status": "ok",
     "timestamp": 1659405027276,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "U8yf75uG6hBW",
    "outputId": "ffe77516-59c0-4de9-96a8-5b39fb1df491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
      "\u001b[?25hCollecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
      "\u001b[K     |████████████████████████████████| 453 kB 70.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUMXvK5H1G9H"
   },
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miXrjR316mNb"
   },
   "source": [
    "* 처리에 필요한 각종 변수 선언\n",
    "* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5292,
     "status": "ok",
     "timestamp": 1659405040921,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "SMjn5PfE1GZR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "filters = \"([~.,!?\\\"':;)(])\"\n",
    "\n",
    "# 토큰\n",
    "PAD = '<PADDING>'\n",
    "STD = '<START>'\n",
    "END = '<END>'\n",
    "UNK = '<UNKNOWN>'\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmRFuH2r6oNJ"
   },
   "source": [
    "* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1659405151495,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "CmrmdXkePWYb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(data_path):\n",
    "    data_df = pd.read_csv(data_path, header = 0)\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer,\n",
    "                                                                        test_size = 0.33,\n",
    "                                                                        random_state = 111)\n",
    "    return train_input, eval_input, train_label, eval_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHuOJHPtPXqq"
   },
   "source": [
    "* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1659405352166,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "QtQL-AP06oSa"
   },
   "outputs": [],
   "source": [
    "def load_vocabulary(data_path):\n",
    "    data_df = pd.read_csv(data_path, encoding = 'utf-8')\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    if tokenize_as_morph:\n",
    "        question = prepro_like_morphlized(question)\n",
    "        answer = prepro_like_morphlized(answer)\n",
    "\n",
    "    data = []\n",
    "    data.extend(question)\n",
    "    data.extend(answer)\n",
    "    words = data_tokenizer(data)\n",
    "    words = list(set(words))\n",
    "    words[:0] = MARKER\n",
    "\n",
    "    char2idx = {char:idx for idx, char in enumerate(words)}\n",
    "    idx2char = {idx:char for idx, char in enumerate(words)}\n",
    "    return char2idx, idx2char, len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wYtpjv76r5q"
   },
   "source": [
    "* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1659405479338,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "-bQ3FOva6tg6"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "def prepro_like_morphlized(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = list()\n",
    "    for seq in data:\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhsVp4pWPTR3"
   },
   "source": [
    "* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1659405480497,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "otLI_RUfPR_g"
   },
   "outputs": [],
   "source": [
    "def data_tokenizer(data):\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkKPA-Mx6uaC"
   },
   "source": [
    "* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1659405917324,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "jK-yeSThPGsa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def enc_processing(value, dictionary):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        if len(sequence_index) > max_len:\n",
    "            sequence_index = sequence_index[:max_len]\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4mM57_FPIg7"
   },
   "source": [
    "* decoder의 입력을 구성하기 위한 함수 `dec_output_processing()` 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1659405925491,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "cX_NpcTq6vw6"
   },
   "outputs": [],
   "source": [
    "def dec_output_processing(value, dictionary):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        if len(sequence_index) > max_len:\n",
    "            sequence_index = sequence_index[:max_len]\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otsTEt4FPLJX"
   },
   "source": [
    "* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1659405940792,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "eeP0PWHEPMma"
   },
   "outputs": [],
   "source": [
    "def dec_target_processing(value, dictionary):\n",
    "    sequences_target_index = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        if len(sequence_index) > max_len:\n",
    "            sequence_index = sequence_index[:max_len - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb9vVUng6xDq"
   },
   "source": [
    "* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n",
    "* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n",
    "* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1659405735985,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "uAlKV4xF62Uf"
   },
   "outputs": [],
   "source": [
    "def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n",
    "    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((train_input_enc, train_output_enc, train_target_dec))\n",
    "    dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n",
    "    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_enc, eval_target_dec))\n",
    "    dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def rearrange(input, output, target):\n",
    "    features = {'input':input, 'output':output}\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is-GhUDN62xC"
   },
   "source": [
    "* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n",
    "* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1659405750255,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "jCfwWXhb64Cc"
   },
   "outputs": [],
   "source": [
    "def pred2string(value, dictionary):\n",
    "    sentence_string = []\n",
    "    is_finished = False\n",
    "\n",
    "    for v in value:\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    answer = \"\"\n",
    "    for word in sentence_string:\n",
    "        if word == END:\n",
    "            if_finished = True\n",
    "            break\n",
    "\n",
    "        if word != PAD and word != END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "    return answer, is_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwp9Nnwz7UoG"
   },
   "source": [
    "* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n",
    "* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "error",
     "timestamp": 1659405763729,
     "user": {
      "displayName": "김지현",
      "userId": "05840969086042947577"
     },
     "user_tz": -540
    },
    "id": "-T536MdU7Taq",
    "outputId": "ec311553-ed50-4fcd-f8f2-c56c5c66af7d"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ce9dcbb9f7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mchar2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-84259c165e04>\u001b[0m in \u001b[0;36mload_vocabulary\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenize_as_morph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepro_like_morphlized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokenize_as_morph = True\n",
    "\n",
    "data_path = 'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv'\n",
    "\n",
    "char2idx, idx2char, len_vocab = load_vocabulary(data_path)\n",
    "train_input, train_label, eval_input, eval_label = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cVd7AOKinqn"
   },
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqLJ0a6r49yi"
   },
   "source": [
    "* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNeeXoZginvj"
   },
   "outputs": [],
   "source": [
    "def model(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimatorlModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimatorlModeKeys.PREDICT\n",
    "\n",
    "    position_encode = positional_encoding(params['embedding_size'], params['max_len'])\n",
    "    if params['xavier_initializer']:\n",
    "        embedding_initializer = 'glorot_normal'\n",
    "    else:\n",
    "        embedding_initializer = 'uniform'\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(params['len_vocab'],\n",
    "                                          params['embedding_size'],\n",
    "                                          embeddings_initializer = embedding_initializer)\n",
    "    \n",
    "    x_embedded_matrix = embedding(features['input']) + position_encode\n",
    "    y_embedded_matrix = embedding(features['output']) + position_encode\n",
    "\n",
    "    encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                              params['attention_head_size'], params['layer_size'])\n",
    "    decoder_outputs = encoder(y_embedded_matrix, encoder_outputs, param['model_hidden_size'],\n",
    "                              params['ffn_hidden_size'], params['attention_head_size'], params['layer_size'])\n",
    "    \n",
    "    logits = tf.keras.layer.Dense(params['len_vocab'])(decoder_outputs)\n",
    "    predict = tf.argmax(logits, 2)\n",
    "\n",
    "    if PREDICT:\n",
    "        predictions = {'indexs':predict,\n",
    "                       'logits':logits}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions = predictions)\n",
    "\n",
    "    labels_ = tf.one_hot(labels, params['len_vocab'])\n",
    "    loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels_))\n",
    "    accuracy = tf.compat.v1.metrics.accuracy(labels = labels, predictions = predict)\n",
    "\n",
    "    matrics = {'accuracy':accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss = loss, eval_metrics_ops = metrics)\n",
    "    assert TRAIN\n",
    "\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = params['learning_rate'])\n",
    "    train_op = optimizer.minimize(loss, global_step = tf.compat.v1.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss = loss, train_op = train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7PrLEWE1JCs"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy_Opm_A7DKC"
   },
   "source": [
    "*   필요한 각종 인자들을 설정\n",
    "*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKGYuqmH6_kj"
   },
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "epoch = 5000\n",
    "batch_size = 256\n",
    "embedding_size = 100\n",
    "model_hidden_size = 100\n",
    "ffn_hidden_size = 100\n",
    "attention_head_size = 100\n",
    "lr = 0.001\n",
    "layer_size = 3\n",
    "xavier_initializer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaXalEy57ODq"
   },
   "source": [
    "*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n",
    "*   평가 데이터에도 동일하게 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWlgWWIq1KSh"
   },
   "outputs": [],
   "source": [
    "train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n",
    "train_output_dec, train_output_dec_length = dec_output_processing(train_input, char2idx)\n",
    "train_target_dec = dec_target_processing(train_label, char2idx)\n",
    "\n",
    "eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx)\n",
    "eval_output_dec, eval_output_dec_length = dec_output_processing(eval_input, char2idx)\n",
    "eval_target_dec = dec_target_processing(eval_label, char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZGgZzWs7Mr7"
   },
   "source": [
    "* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n",
    "* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9vjc3Ck7F4J"
   },
   "outputs": [],
   "source": [
    "transformer = tf.estimator.Estimator(\n",
    "    model_fn = model,\n",
    "    params = {'embedding_size': embedding_size,\n",
    "              'model_hidden_size': model_hidden_size,\n",
    "              'ffn_hidden_size': ffn_hidden_size,\n",
    "              'attention_head_size': attention_head_size,\n",
    "              'learning_rate': lr,\n",
    "              'len_vocab': len_vocab,\n",
    "              'layer_size': layer_size,\n",
    "              'max_len': max_len,\n",
    "              'xavier_initializer': xavier_initializer}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl_pwUiw7INZ"
   },
   "source": [
    "* 학습한 모델을 사용해 챗봇을 사용\n",
    "* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n",
    "* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COO-0PcS7Hy5"
   },
   "outputs": [],
   "source": [
    "transformer.train(input_fn = lambda: train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size), steps = epoch)\n",
    "eval_result = transformer.evaluate(input_fn = lambda: eval_input_fn(train_input_enc, eval_output_dec, eval_target_dec, batch_size))\n",
    "\n",
    "print(**eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNcrVf2z1LSM"
   },
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5lY9DrW8eSK"
   },
   "source": [
    "* 학습한 모델을 사용해 챗봇을 사용\n",
    "* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n",
    "* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9IQaBx4Qw8J"
   },
   "outputs": [],
   "source": [
    "def chatbot(sencence):\n",
    "    pred_input_enc, pred_input_enc_length = enc_processing([sentence], char2idx)\n",
    "    pred_output_dec, pred_output_dec_length = dec_output_processing([\"\"], char2idx)\n",
    "    pred_target_dec = dec_target_processing([\"\"], char2idx)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        if i > 0:\n",
    "            pred_output_dec, pred_output_dec_length = dec_output_processing([answer], char2idx)\n",
    "            pred_target_dec = dec_target_processing([answer], char2idx)\n",
    "\n",
    "        predictions = transformer.predict(input_fn = lambda: eval_input_fn(pred_input_enc, pred_output_dec, pred_target_dec, 1))\n",
    "\n",
    "        answer, finished = pred2string(predictions, idx2char)\n",
    "\n",
    "        if finished:\n",
    "            break\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjHZKvJ31MAU"
   },
   "outputs": [],
   "source": [
    "chatbot(\"안녕?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mjRZwyLQ_gP"
   },
   "outputs": [],
   "source": [
    "chatbot(\"너 누구냐?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7AJCsXRTqJx"
   },
   "outputs": [],
   "source": [
    "chatbot(\"뭐 먹었어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M8mfoUfeAWQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5mrdGRaem6v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13 트랜스포머 (Transformer).ipynb",
   "provenance": [
    {
     "file_id": "1qBC_BPdmQgTWSd6n1B4d-pNAaATy_xVC",
     "timestamp": 1658468331745
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
